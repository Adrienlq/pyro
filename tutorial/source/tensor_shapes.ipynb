{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor shapes in Pyro 0.2\n",
    "\n",
    "This tutorial introduces Pyro's organization of tensor dimensions. Before starting, you should familiarize yourself with [PyTorch broadcasting semantics](http://pytorch.org/docs/master/notes/broadcasting.html).\n",
    "\n",
    "#### Summary:\n",
    "- While you are learning or debugging, set `pyro.enable_validation(True)`.\n",
    "- Tensors broadcast by aligning on the right: `torch.ones(3,4,5) + torch.ones(5)`.\n",
    "- Distribution `.sample().shape == batch_shape + event_shape`.\n",
    "- Distribution `.log_prob(x).shape == batch_shape` (but not `event_shape`!).\n",
    "- Use `my_dist.reshape([2,3,4])` to draw a batch of samples.\n",
    "- Use `my_dist.reshape(extra_event_dims=1)` to declare a dimension as dependent.\n",
    "- Use `with pyro.iarange('name', size):` to declare a dimension as independent.\n",
    "- All dimensions must be declared either dependent or independent.\n",
    "- Try to support batching on the left. This lets Pyro auto-parallelize.\n",
    "  - use negative indices like `x.sum(-1)` rather than `x.sum(2)`\n",
    "  - use ellipsis notation like `pixel = image[..., i, j]`\n",
    "  \n",
    "#### Table of Contents\n",
    "- [Distribution shapes](#dist-shapes)\n",
    "  - [Examples](#dist-examples)\n",
    "  - [Reshaping distributions](#dist-reshape)\n",
    "- [Declaring independence with `iarange`](#iarange)\n",
    "- [Subsampling inside `iarange`](#subsampling)\n",
    "- [Broadcasting to allow Parallel Enumeration](#subsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.distributions import Bernoulli, Categorical, MultivariateNormal, Normal\n",
    "from pyro.infer import ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)    # <---- This is always a good idea!\n",
    "\n",
    "def test_model(model, guide=None, **kwargs):\n",
    "    pyro.clear_param_store()\n",
    "    ELBO.make(**kwargs).loss(model, model if guide is None else guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions shapes: `batch_shape` and `event_shape` <a class=\"anchor\" id=\"dist-shapes\"></a>\n",
    "\n",
    "PyTorch `Tensor`s have a single `.shape` attribute, but `Distribution`s have two shape attributions with special meaning: `.batch_shape` and `.event_shape`. These two combine to define the total shape of a sample\n",
    "```py\n",
    "x = d.sample()\n",
    "assert x.shape == d.batch_shape + d.event_shape\n",
    "```\n",
    "Indices over `.batch_shape` denote independent random variables, whereas indices over `.event_shape` denote dependent random variables. Because the dependent random variables define probability together, the `.log_prob()` method only produces a single number for each event of shape `.event_shape`. Thus the total shape of `.log_prob()` is `.batch_shape`:\n",
    "```py\n",
    "assert d.log_prob(x).shape == d.batch_shape\n",
    "```\n",
    "Note that the `.sample()` method also takes `sample_shape` parameter that indexes over independent identically distributed (iid) random varables, so that\n",
    "```py\n",
    "x2 = d.sample(sample_shape)\n",
    "assert x2.shape == sample_shape + batch_shape + event_shape\n",
    "```\n",
    "In summary\n",
    "```\n",
    "      |      iid     | independent | dependent\n",
    "------+--------------+-------------+------------\n",
    "shape = sample_shape + batch_shape + event_shape\n",
    "```\n",
    "Note that univariate distributions have empty event shape (because each number is an independent event). Distributions over vectors like `MultivariateNormal` have `len(event_shape) == 1`. Distributions over matrices like `InverseWishart` have `len(event_sahpe) == 2`.\n",
    "\n",
    "### Examples <a class=\"anchor\" id=\"dist-examples\"></a>\n",
    "\n",
    "The simplest distribution shape is a single univariate distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == ()\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions can be batched by passing in batched parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5 * torch.ones(3,4))\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to batch distributions is via the `.reshape()` method. This only works if \n",
    "parameters are identical along the leftmost dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).reshape([3])\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate distributions have nonempty `.event_shape`. For these distributions, the shapes of `.sample()` and `.log_prob(x)` differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,)\n",
    "x = d.sample()\n",
    "assert x.shape == (3,)            # == batch_shape + event_shape\n",
    "assert d.log_prob(x).shape == ()  # == batch_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping distributions <a class=\"anchor\" id=\"dist-reshape\"></a>\n",
    "\n",
    "In Pyro you can treat a univariate distribution as multivariate by calling the `.reshape(extra_event_dims=_)` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5 * torch.ones(3,4)).reshape(extra_event_dims=1)\n",
    "assert d.batch_shape == (3,)\n",
    "assert d.event_shape == (4,)\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you work with Pyro programs, keep in mind that samples have shape `batch_shape + event_shape`, whereas `.log_prob(x)` values have shape `batch_shape`. You'll need to ensure that `batch_shape` is carefully controlled by either trimming it down with `.reshape(extra_event_dims=n)` or by declaring dimensions as independent via `pyro.iarange`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring independent dims with `iarange` <a class=\"anchor\" id=\"iarange\"></a>\n",
    "\n",
    "Pyro models can use the context manager [pyro.iarange](http://docs.pyro.ai/en/dev/primitives.html#pyro.__init__.iarange) to declare that certain batch dimensions are independent. Inference algorithms can then take advantage of this independence to e.g. construct lower variance gradient estimators or to enumerate in linear space rather than exponential space. An example of an independent dimension is the index over data in a minibatch: each datum should be independent of all others.\n",
    "\n",
    "The simplest way to declare a dimension as independent is to declare the rightmost batch dimension as independent via a simple\n",
    "```py\n",
    "with pyro.iarange(\"my_iarange\"):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "```\n",
    "We recommend always providing an optional size argument to aid in debugging shapes\n",
    "```py\n",
    "with pyro.iarange(\"my_iarange\", len(my_data)):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "```\n",
    "Starting with Pyro 0.2 you can additionally nest `iaranges`, e.g. if you have per-pixel independence:\n",
    "```py\n",
    "with pyro.iarange(\"x_axis\", 320):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "    with pyro.iarange(\"y_axis\", 200):\n",
    "        # within this context, batch dimensions -2 and -1 are independent\n",
    "```\n",
    "Note that we always count from the right by using negative indices like -2, -1.\n",
    "\n",
    "Finally if you want to mix and match `iarange`s for e.g. noise that depends only on `x`, some noise that depends only on `y`, and some noise that depends on both, you can declare multiple `iaranges` and use them as reusable context managers. In this case Pyro cannot automatically allocate a dimension, so you need to provide a `dim` argument (again counting from the right):\n",
    "```py\n",
    "x_axis = pyro.iarange(\"x_axis\", 3, dim=-2)\n",
    "y_axis = pyro.iarange(\"y_axis\", 2, dim=-3)\n",
    "with x_axis:\n",
    "    # within this context, batch dimension -2 is independent\n",
    "with y_axis:\n",
    "    # within this context, batch dimension -3 is independent\n",
    "with x_axis, y_axis:\n",
    "    # within this context, batch dimensions -3 and -2 are independent\n",
    "```\n",
    "Let's take a closer look at batch sizes within `iarange`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    a = pyro.sample(\"a\", Normal(0, 1))\n",
    "    b = pyro.sample(\"b\", Normal(torch.zeros(2), 1).reshape(extra_event_dims=1))\n",
    "    with pyro.iarange(\"c_iarange\", 2):\n",
    "        c = pyro.sample(\"c\", Normal(torch.zeros(2), 1))\n",
    "    with pyro.iarange(\"d_iarange\", 3):\n",
    "        d = pyro.sample(\"d\", Normal(torch.zeros(3,4,5), 1).reshape(extra_event_dims=2))\n",
    "    assert a.shape == ()       # batch_shape == ()     event_shape == ()\n",
    "    assert b.shape == (2,)     # batch_shape == ()     event_shape == (2,)\n",
    "    assert c.shape == (2,)     # batch_shape == (2,)   event_sahpe == ()\n",
    "    assert d.shape == (3,4,5)  # batch_shape == (3,)   event_shape == (4,5) \n",
    "\n",
    "    x_axis = pyro.iarange(\"x_axis\", 3, dim=-2)\n",
    "    y_axis = pyro.iarange(\"y_axis\", 2, dim=-3)\n",
    "    with x_axis:\n",
    "        x = pyro.sample(\"x\", Normal(0, 1).reshape([3, 1]))\n",
    "    with y_axis:\n",
    "        y = pyro.sample(\"y\", Normal(0, 1).reshape([2, 1, 1]))\n",
    "    with x_axis, y_axis:\n",
    "        xy = pyro.sample(\"xy\", Normal(0, 1).reshape([2, 3, 1]))\n",
    "        z = pyro.sample(\"z\", Normal(0, 1).reshape([2, 3, 1, 5], 1))\n",
    "    assert x.shape == (3, 1)        # batch_shape == (3,1)     event_shape == ()\n",
    "    assert y.shape == (2, 1, 1)     # batch_shape == (2,1,1)   event_shape == ()\n",
    "    assert xy.shape == (2, 3, 1)    # batch_shape == (2,3,1)   event_shape == ()\n",
    "    assert z.shape == (2, 3, 1, 5)  # batch_shape == (2,3,1)   event_shape == (5,)\n",
    "    \n",
    "test_model(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to visualize the `.shape`s of each sample site by aligning them at the boundary between `batch_shape` and `event_shape`: dimensions to the right will be summed out in `.log_prob()` and dimensions to the left will remain. \n",
    "```\n",
    "batch dims | event dims\n",
    "-----------+-----------\n",
    "           |        a = sample(\"a\", Normal(0, 1))\n",
    "           |2       b = sample(\"b\", Normal(zeros(2), 1)\n",
    "           |                        .reshape(extra_event_dims=1)\n",
    "           |        with iarange(\"c\", 2):\n",
    "          2|            c = sample(\"c\", Normal(zeros(2), 1))\n",
    "           |        with iarange(\"d\", 3):\n",
    "          3|4 5         d = sample(\"d\", Normal(zeros(3,4,5), 1)\n",
    "           |                       .reshape(extra_event_dims=2)\n",
    "           |\n",
    "           |        x_axis = iarange(\"x\", 3, dim=-2)\n",
    "           |        y_axis = iarange(\"y\", 2, dim=-3)\n",
    "           |        with x_axis:\n",
    "        3 1|            x = sample(\"x\", Normal(0, 1).reshape([3, 1]))\n",
    "           |        with y_axis:\n",
    "      2 1 1|            y = sample(\"y\", Normal(0, 1).reshape([2, 1, 1]))\n",
    "           |        with x_axis, y_axis:\n",
    "      2 3 1|            xy = sample(\"xy\", Normal(0, 1).reshape([2, 3, 1]))\n",
    "      2 3 1|5           z = sample(\"z\", Normal(0, 1).reshape([2, 3, 1, 5], 1))\n",
    "```\n",
    "As an exercise, try to tabulate the shapes of sample sites in one of your own programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling tensors inside an `iarange`    <a class=\"anchor\" id=\"subsampling\"></a>\n",
    "\n",
    "One of the main uses of [iarange](http://docs.pyro.ai/en/dev/primitives.html#pyro.__init__.iarange) is to subsample data. This is possible within an `iarange` because data are independent, so the expected value of the loss on half the data should be half the expected value of the loss on the full data.\n",
    "\n",
    "To subsample data, you need to inform Pyro of both the original data size and the subsample size; Pyro will then choose a random subset of data and yield the set of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(100)\n",
    "\n",
    "def model2():\n",
    "    mean = pyro.param(\"mean\", torch.zeros(len(data)))\n",
    "    with pyro.iarange(\"data\", len(data), subsample_size=10) as ind:\n",
    "        assert len(ind) == 10    # ind is a LongTensor that indexes the subsample.\n",
    "        batch = data[ind]        # Select a minibatch of data.\n",
    "        mean_batch = mean[ind]   # Take care to select the relevant per-datum parameters.\n",
    "        # Do stuff with batch:\n",
    "        x = pyro.sample(\"x\", Normal(mean_batch, 1), obs=batch)\n",
    "        assert len(x) == 10\n",
    "        \n",
    "test_model(model2, guide=lambda: None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting to allow parallel enumeration <a class=\"anchor\" id=\"enumerate\"></a>\n",
    "\n",
    "Pyro 0.2 introduces the ability to enumerate discrete latent variables in parallel. This can significantly reduce the variance of gradient estimators when fitting an SVI model.\n",
    "\n",
    "To use discrete enumeration, Pyro needs to allocate tensor dimension that it can use for enumeration. To avoid conflicting with other dimensions that we want to use for `iarange`s, we need to declare a budget of the maximum number of tensor dimensions we'll use. This budget is called `max_iarange_nesting` and is an argument to [SVI](http://docs.pyro.ai/en/dev/inference_algos.html) (the argument is simply passed through to [TraceEnum_ELBO](http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.traceenum_elbo.TraceEnum_ELBO)).\n",
    "\n",
    "To understand `max_iarange_nesting` and how Pyro allocates dimensions for enumeration, let's revisit `model1()` from above. This time we'll map out three types of dimensions:\n",
    "enumeration dimensions on the left (Pyro takes control of these), batch dimensions in the middle, and event dimensions on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      max_iarange_nesting = 3\n",
    "           |<--->|\n",
    "enumeration|batch|event\n",
    "-----------+-----+-----\n",
    "           |. . .|      a = sample(\"a\", Normal(0, 1))\n",
    "           |. . .|2     b = sample(\"b\", Normal(zeros(2), 1)\n",
    "           |     |                      .reshape(extra_event_dims=1))\n",
    "           |     |      with iarange(\"c\", 2):\n",
    "           |. . 2|          c = sample(\"c\", Normal(zeros(2), 1))\n",
    "           |     |      with iarange(\"d\", 3):\n",
    "           |. . 3|4 5       d = sample(\"d\", Normal(zeros(3,4,5), 1)\n",
    "           |     |                     .reshape(extra_event_dims=2))\n",
    "           |     |\n",
    "           |     |      x_axis = iarange(\"x\", 3, dim=-2)\n",
    "           |     |      y_axis = iarange(\"y\", 2, dim=-3)\n",
    "           |     |      with x_axis:\n",
    "           |. 3 1|          x = sample(\"x\", Normal(0, 1).reshape([3,1]))\n",
    "           |     |      with y_axis:\n",
    "           |2 1 1|          y = sample(\"y\", Normal(0, 1).reshape([2,1,1]))\n",
    "           |     |      with x_axis, y_axis:\n",
    "           |2 3 1|          xy = sample(\"xy\", Normal(0, 1).reshape([2,3,1]))\n",
    "           |2 3 1|5         z = sample(\"z\", Normal(0, 1).reshape([2,3,1,5], 1))\n",
    "```\n",
    "Note that we can overprovision `max_iarange_nesting=4` but we cannot underprovision `max_iarange_nesting=2` (or Pyro will error). Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@config_enumerate(default=\"parallel\")\n",
    "def model3():\n",
    "    p = pyro.param(\"p\", torch.arange(6) / 6)\n",
    "    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n",
    "\n",
    "    a = pyro.sample(\"a\", Categorical(torch.ones(6) / 6))\n",
    "    b = pyro.sample(\"b\", Bernoulli(p[a]))  # Note this depends on a.\n",
    "    with pyro.iarange(\"c_iarange\", 4):\n",
    "        c = pyro.sample(\"c\", Bernoulli(0.3).reshape([4]))\n",
    "        with pyro.iarange(\"d_iarange\", 5):\n",
    "            d = pyro.sample(\"d\", Bernoulli(0.4).reshape([5,4]))\n",
    "            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            e_scale = torch.arange(1, 8)\n",
    "            e = pyro.sample(\"e\", Normal(e_loc, e_scale)\n",
    "                            .reshape(extra_event_dims=1))  # Note this depends on d.\n",
    "\n",
    "    #                   enumerated|batch|event dims\n",
    "    assert a.shape == (         6, 1, 1   )  # Six enumerated values of the Categorical.\n",
    "    assert b.shape == (      2, 6, 1, 1   )  # 2 enumerated Bernoullis x 6 Categoricals.\n",
    "    assert c.shape == (   2, 1, 1, 1, 4   )  # Only 2 Bernoullis; does not depend on a or b.\n",
    "    assert d.shape == (2, 1, 1, 1, 5, 4   )  # Only two Bernoullis.\n",
    "    assert e.shape == (2, 1, 1, 1, 5, 4, 7)  # This is sampled and depends on d.\n",
    "\n",
    "    assert e_loc.shape   == (2, 1, 1, 1, 5, 4, 1,)\n",
    "    assert e_scale.shape == (                  7,)\n",
    "            \n",
    "test_model(model3, max_iarange_nesting=2, enum_discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at those dimensions. First note that Pyro allocates enumeration dims starting from the right at `max_iarange_nesting`: Pyro allocates dim -3 to enumerate `a`, then dim -4 to enumerate `b`, then dim -5 to enumerate `c`, and finally dim -6 to enumerate `d`. Next note that variables only have extent (size > 1) in dimensions they depend on. This helps keep tensors small and computation cheap. We can draw a similar map of the tensor dimensions:\n",
    "```\n",
    "     max_iarange_nesting = 2\n",
    "            |<->|\n",
    "enumeration batch event\n",
    "------------|---|-----\n",
    "           6|1 1|     a = pyro.sample(\"a\", Categorical(torch.ones(6) / 6))\n",
    "         2 1|1 1|     b = pyro.sample(\"b\", Bernoulli(p[a]))\n",
    "            |   |     with pyro.iarange(\"c_iarange\", 4):\n",
    "       2 1 1|1 4|         c = pyro.sample(\"c\", Bernoulli(0.3).reshape([4]))\n",
    "            |   |         with pyro.iarange(\"d_iarange\", 5):\n",
    "     2 1 1 1|5 4|             d = pyro.sample(\"d\", Bernoulli(0.4).reshape([5,4]))\n",
    "     2 1 1 1|5 4|1            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            |   |7            e_scale = torch.arange(1, 8)\n",
    "     2 1 1 1|5 4|7            e = pyro.sample(\"e\", Normal(e_loc, e_scale)\n",
    "            |   |                             .reshape(extra_event_dims=1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
